{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3HqYZdfA0oD"
   },
   "source": [
    "#**Prompt Engineering**\n",
    "\n",
    "In this lab we are going to gain some practice with using LLM assistants and with prompt engineering.\n",
    "\n",
    "We will be do this with Llama2, an open source LLM, and  the llama2.cpp interface. You will have to write prompts to carry out several NLP tasks studied in the module.\n",
    "\n",
    "It is recommended you try to connect to a T4 GPU on Colab as this will speed up things considerably.\n",
    "\n",
    "The part of the  lab dedicated to setting up the interface is based on the HuggingFace lab https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/discussions/3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bKQIsIq-d8y"
   },
   "source": [
    "#Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnV5UC7A2vBZ"
   },
   "source": [
    "Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases. In this lab we will be using Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAXhw_WHrXip"
   },
   "source": [
    "#Llama2.cpp\n",
    "\n",
    "`llama.cpp` can be used to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdr1W8P2em3P"
   },
   "source": [
    "Task 0: Please read the description of the library at https://llama-cpp-python.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubyTJi01e47w"
   },
   "source": [
    "# Setting up llama cpp python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odwxcng9r_S8"
   },
   "source": [
    "The library works the same with a CPU, but the inference can take about three times longer compared to using it on a GPU.\n",
    "\n",
    "If you want to use only the CPU, you can replace the content of the cell below with the following lines.\n",
    "```\n",
    "# CPU llama-cpp-python\n",
    "!pip install llama-cpp-python==0.1.78\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rkBmY3vQvRSw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting llama-cpp-python==0.1.78\n",
      "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l  Running command pip subprocess to install build dependencies\n",
      "  Collecting setuptools>=42\n",
      "    Downloading setuptools-79.0.1-py3-none-any.whl (1.3 MB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 24.7 MB/s eta 0:00:00\n",
      "  Collecting scikit-build>=0.13\n",
      "    Downloading scikit_build-0.18.1-py3-none-any.whl (85 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 KB 28.1 MB/s eta 0:00:00\n",
      "  Collecting cmake>=3.18\n",
      "    Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 27.9/27.9 MB 94.6 MB/s eta 0:00:00\n",
      "  Collecting ninja\n",
      "    Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 422.8/422.8 KB 95.5 MB/s eta 0:00:00\n",
      "  Collecting wheel>=0.32.0\n",
      "    Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.5/72.5 KB 22.0 MB/s eta 0:00:00\n",
      "  Collecting distro\n",
      "    Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "  Collecting tomli\n",
      "    Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "  Collecting packaging\n",
      "    Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 KB 24.0 MB/s eta 0:00:00\n",
      "  Installing collected packages: wheel, tomli, setuptools, packaging, ninja, distro, cmake, scikit-build\n",
      "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "  pipx 1.0.0 requires argcomplete>=1.9.4, but you have argcomplete 1.8.1 which is incompatible.\n",
      "  Successfully installed cmake-4.0.0 distro-1.9.0 ninja-1.11.1.4 packaging-25.0 scikit-build-0.18.1 setuptools-79.0.1 tomli-2.2.1 wheel-0.45.1\n",
      "\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l  Running command Getting requirements to build wheel\n",
      "  running egg_info\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "  running dist_info\n",
      "  creating /tmp/pip-modern-metadata-3_8c50yv/llama_cpp_python.egg-info\n",
      "  writing manifest file '/tmp/pip-modern-metadata-3_8c50yv/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  writing manifest file '/tmp/pip-modern-metadata-3_8c50yv/llama_cpp_python.egg-info/SOURCES.txt'\n",
      "\u001b[?25hdone\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-2.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m299.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 KB\u001b[0m \u001b[31m390.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 KB\u001b[0m \u001b[31m386.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "\n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "\n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Running\n",
      "\n",
      "     'ninja' '--version'\n",
      "\n",
      "    failed with:\n",
      "\n",
      "     no such file or directory\n",
      "\n",
      "\n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Unix Makefiles' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "\n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Configuring done (0.3s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_cmake_test_compile/build\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Unix Makefiles' generator - success\n",
      "  --------------------------------------------------------------------------------\n",
      "\n",
      "  Configuring Project\n",
      "    Working directory:\n",
      "      /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "    Command:\n",
      "      /tmp/pip-build-env-qpdaxzeo/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79 -G 'Unix Makefiles' --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-qpdaxzeo/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython_NumPy_INCLUDE_DIRS:PATH=/usr/lib/python3/dist-packages/numpy/core/include -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_NumPy_INCLUDE_DIRS:PATH=/usr/lib/python3/dist-packages/numpy/core/include -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n",
      "\n",
      "  Not searching for unused variables given on the command line.\n",
      "  -- The C compiler identification is GNU 11.4.0\n",
      "  -- The CXX compiler identification is GNU 11.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /bin/git (found version \"2.34.1\")\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  fatal: not a git repository (or any of the parent directories): .git\n",
      "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n",
      "    Git repository not found; to enable automatic generation of build info,\n",
      "    make sure Git is installed and the project is a Git repository.\n",
      "\n",
      "\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Unable to find cuda_runtime.h in \"//include\" for CUDAToolkit_INCLUDE_DIRECTORIES.\n",
      "  -- Found CUDAToolkit: /usr/include (found version \"12.8.61\")\n",
      "  -- cuBLAS found\n",
      "  -- The CUDA compiler identification is NVIDIA 12.8.61 with host compiler GNU 11.4.0\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- Using CUDA architectures: 52;61;70\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- x86 detected\n",
      "  -- Configuring done (2.2s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-build\n",
      "  [ 12%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n",
      "  [ 25%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n",
      "  [ 37%] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
      "  nvcc warning : Support for offline compilation for architectures prior to '<compute/sm/lto>_75' will be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [ 50%] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n",
      "  [ 50%] Built target ggml\n",
      "  [ 62%] Linking CUDA static library libggml_static.a\n",
      "  [ 62%] Built target ggml_static\n",
      "  [ 75%] Linking CUDA shared library libggml_shared.so\n",
      "  [ 75%] Built target ggml_shared\n",
      "  [ 87%] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n",
      "  [100%] Linking CXX shared library libllama.so\n",
      "  [100%] Built target llama\n",
      "  Install the project...\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n",
      "  -- Installing: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n",
      "  -- Installing: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n",
      "  -- Installing: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n",
      "\n",
      "  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n",
      "  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n",
      "  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n",
      "  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n",
      "  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n",
      "  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n",
      "  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n",
      "  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n",
      "  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n",
      "  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n",
      "  copying /tmp/pip-install-piqeaga_/llama-cpp-python_d420c9435ca0463ca77f4ecf5cee8b79/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n",
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp\n",
      "  running build_ext\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n",
      "  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-3.10/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n",
      "  running install_data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n",
      "  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n",
      "  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.egg-info\n",
      "  running install_scripts\n",
      "\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=6208159 sha256=a1595a46b4d174ae6590a1c70105f6e73accf7d1a5ea704689a6f44b71cefada\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-nmjggryy/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
      "  changing mode of /home/ubuntu/.local/bin/f2py to 775\n",
      "  changing mode of /home/ubuntu/.local/bin/numpy-config to 775\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-2.2.5 typing-extensions-4.13.2\n"
     ]
    }
   ],
   "source": [
    "# GPU llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "b_nBHTYSoIWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/lib/python3/dist-packages (from huggingface_hub) (21.3)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 KB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface_hub) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub) (3.6.0)\n",
      "Installing collected packages: tqdm, huggingface_hub\n",
      "Successfully installed huggingface_hub-0.30.2 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# To download the models\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R11KqY7lW0yv"
   },
   "source": [
    "# Choosing the Llama2 version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzJkCoRRbICP"
   },
   "source": [
    "\n",
    "Next, we need to specify which version of Llama2 to use. In Colab with T4 GPU, we can run models of up to 20B of parameters with all optimizations, but this may degrade the quality of the model's inference. The library can run GGML models on a CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHAz1yGZb4lq"
   },
   "source": [
    "In this lab, we will use  [Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)\n",
    "\n",
    "![asd](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc24dac6d-6b5e-4b5f-938c-05951c938a9e_1085x543.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSeedwAFSay9"
   },
   "source": [
    "#  Quantized Models from the Hugging Face Community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QwfjGFYRM4g"
   },
   "source": [
    "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
    "\n",
    "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
    "\n",
    "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
    "\n",
    "\n",
    "\n",
    "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L7AZFe_7Px0"
   },
   "source": [
    "The prefix 'q5_1' signifies the quantization method we used. To determine the best method in each case, one rule is that 'q8' yields superior responses at the cost of higher memory usage [slow]. On the other hand, 'q2' may generate subpar responses but requires less RAM [fast].\n",
    "\n",
    "There are other quantization methods available, and you can read about them in the [model card](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oI-kXwg5bHF-"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Td05XSuiWdI"
   },
   "source": [
    "We download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cBEJr-G-2ht4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "max1jwxvCSbm"
   },
   "source": [
    "# Inference with llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TOfnZpj394g"
   },
   "source": [
    "Setting up the interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "I7BvKO0lv_Wc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A10, compute capability 8.6\n",
      "llama.cpp: loading model from /home/ubuntu/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/3140827b4dfcb6b562cd87ee3d7f07109b014dd0/llama-2-13b-chat.ggmlv3.q5_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 4096\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 9 (mostly Q5_1)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "llama_model_load_internal: mem required  =  782.30 MB (+ 3200.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x (640 kB + n_ctx x 160 B) = 640 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 40 repeating layers to GPU\n",
      "llama_model_load_internal: offloading non-repeating layers to GPU\n",
      "llama_model_load_internal: offloading v cache to GPU\n",
      "llama_model_load_internal: offloading k cache to GPU\n",
      "llama_model_load_internal: offloaded 43/43 layers to GPU\n",
      "llama_model_load_internal: total VRAM used: 13034 MB\n",
      "llama_new_context_with_model: kv self size  = 3200.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# GPU\n",
    "from llama_cpp import Llama\n",
    "lcpp_llm = None\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=43, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096, # Context window\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdZnPtB8-Bhx"
   },
   "source": [
    "\n",
    "To run in CPU\n",
    "```\n",
    "# CPU\n",
    "from llama_cpp import Llama\n",
    "\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    )\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BeH6eWiKuaxW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the number of layers in GPU\n",
    "lcpp_llm.params.n_gpu_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFqumvkEm3KJ"
   },
   "source": [
    "# First example of prompt use: generating code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLEEOufGVlID"
   },
   "source": [
    "A zero shot prompt asking Llama2 to write linear regression code in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-NzVIlMCVoVD"
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a linear regression in python\"\n",
    "prompt_template=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaFQjPhcFgfN"
   },
   "source": [
    "Generating response\n",
    "\n",
    "If you only use CPU, the response can take a long time. You can reduce the max_tokens to get a faster response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R76uxL293jTc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a linear regression in python\n",
      "\n",
      "ASSISTANT:\n",
      "\n",
      "To write a linear regression in Python, you can use the scikit-learn library. Here is an example of how to do this:\n",
      "```\n",
      "from sklearn.linear_model import LinearRegression\n",
      "import pandas as pd\n",
      "\n",
      "# Load your dataset into a Pandas DataFrame\n",
      "df = pd.read_csv('your_data.csv')\n",
      "\n",
      "# Create a linear regression object and fit the data\n",
      "reg = LinearRegression().fit(df[['x1', 'x2']], df['y'])\n",
      "\n",
      "# Print the coefficients\n",
      "print(reg.coef_)\n",
      "\n",
      "# Print the R-squared value\n",
      "print(reg.score(df[['x1', 'x2']], df['y']))\n",
      "```\n",
      "This code will load your dataset into a Pandas DataFrame, create a linear regression object and fit the data using the `fit()` method. It will then print the coefficients of the linear regression and the R-squared value, which measures the goodness of fit of the model.\n",
      "\n",
      "Please note that this is just an example code, you need to adjust it according to your dataset and problem. Also, you can use other libraries such as statsmodels, seab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =   129.56 ms /   256 runs   (    0.51 ms per token,  1975.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6322.76 ms /    39 tokens (  162.12 ms per token,     6.17 tokens per second)\n",
      "llama_print_timings:        eval time =  6697.40 ms /   255 runs   (   26.26 ms per token,    38.07 tokens per second)\n",
      "llama_print_timings:       total time = 13540.48 ms\n"
     ]
    }
   ],
   "source": [
    "response = lcpp_llm(\n",
    "    prompt=prompt_template,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpqTD_Ioj4FN"
   },
   "source": [
    "# Second example (also zero shot): a natural language generation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlZk9uaqnW7x"
   },
   "source": [
    "A zero shot prompt asking Llama2 to write a story\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Ey6xRNFMnixL"
   },
   "outputs": [],
   "source": [
    "prompt_nlg = \"Write a story about a bear called Paddington\"\n",
    "prompt_template_nlg=f'''SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
    "\n",
    "USER: {prompt_nlg}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5ERhRzjXn8Vt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM: You are a helpful, respectful and honest assistant. Always answer as helpfully.\n",
      "\n",
      "USER: Write a story about a bear called Paddington\n",
      "\n",
      "ASSISTANT:\n",
      "Once upon a time in Peru, there was a little bear named Paddington who lived with his Aunt Lucy in the Andes Mountains. He loved to eat marmalade and dreamt of going on an adventure to find more of his favorite food. One day, he stowed away on a ship bound for England, where he arrived at Paddington Station with nothing but a suitcase full of marmalade. There, he was taken in by the Brown family who lived nearby. They were kind and welcoming, and soon became like a family to Paddington. With their help, he explored the city, made new friends, and even found a job as a waiter at a local hotel. Despite encountering many challenges along the way, Paddington always remained true to his nature - kind, polite, and with an insatiable love for marmalade. His story has become legendary in England, inspiring generations of children (and adults!) to be as brave, curious, and loving as this beloved bear from Peru.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =   112.64 ms /   223 runs   (    0.51 ms per token,  1979.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   287.51 ms /    15 tokens (   19.17 ms per token,    52.17 tokens per second)\n",
      "llama_print_timings:        eval time =  5815.31 ms /   222 runs   (   26.20 ms per token,    38.18 tokens per second)\n",
      "llama_print_timings:       total time =  6549.12 ms\n"
     ]
    }
   ],
   "source": [
    "response_nlg = lcpp_llm(\n",
    "    prompt=prompt_template_nlg,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response_nlg[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aElKimJpZn7C"
   },
   "source": [
    "# Task 1: natural language generation with zero-shot prompting  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlCHmJ1RcLTW"
   },
   "source": [
    "Task 1: Write a prompt to get Llama2 to generate a recipe for spaghetti al pomodoro. Experiment with different prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tF0K77AjWKZ"
   },
   "source": [
    "Write your prompt below, and call it ```prompt_task1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xDCOTUjHcc0s"
   },
   "outputs": [],
   "source": [
    "prompt_task1 = 'Give a recipe for a classic spaghetti al pomodoro like my nona made'\n",
    "prompt_template_task1 = f'''\n",
    "\n",
    "USER: {prompt_task1}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fam0IgsbeHMO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USER: Give a recipe for a classic spaghetti al pomodoro like my nona made\n",
      "\n",
      "ASSISTANT:\n",
      "Certo! Here's a traditional Italian recipe for spaghetti all'alpomodoro, just like Nonna used to make. This dish is a simple yet flavorful combination of fresh tomatoes, garlic, basil, and olive oil that will transport your taste buds straight to the rolling hills of Tuscany!\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 12 oz (340g) spaghetti\n",
      "* 2 large ripe tomatoes, peeled and chopped\n",
      "* 3 cloves garlic, minced\n",
      "* 1/4 cup extra-virgin olive oil\n",
      "* Salt to taste\n",
      "* Fresh basil leaves, chopped (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a large pot of salted water to a boil and cook the spaghetti according to package instructions until al dente. Reserve 1 cup of pasta water before draining the spaghetti.\n",
      "2. In a blender or food processor, combine the tomatoes, garlic, olive oil, salt, and a pinch of black pepper. Blend until smooth and creamy, stopping to scra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =   129.64 ms /   256 runs   (    0.51 ms per token,  1974.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   297.67 ms /    24 tokens (   12.40 ms per token,    80.63 tokens per second)\n",
      "llama_print_timings:        eval time =  6697.81 ms /   255 runs   (   26.27 ms per token,    38.07 tokens per second)\n",
      "llama_print_timings:       total time =  7518.77 ms\n"
     ]
    }
   ],
   "source": [
    "response_task1 = lcpp_llm(\n",
    "    prompt=prompt_template_task1,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response_task1[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIK0wzcEfj_u"
   },
   "source": [
    "# Task 2: summarization with zero-shot prompting  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa_5ceCffrjk"
   },
   "source": [
    "Task 2: Write a prompt to get llama2 to produce a summary of the following Wikipedia article on Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "H3nXTPBWf1Ws"
   },
   "outputs": [],
   "source": [
    "task2_article = f'''\n",
    "LLaMA\n",
    "\n",
    "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023.\n",
    "\n",
    "For the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters.\n",
    "LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\n",
    "\n",
    "In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.\n",
    "\n",
    "LLaMA-2\n",
    "\n",
    "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA.\n",
    "Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4]\n",
    "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5]\n",
    "The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
    "\n",
    "LLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat.\n",
    "In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases.\n",
    "However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative\n",
    "(known for maintaining the Open Source Definition).[6]\n",
    "\n",
    "Architecture\n",
    "\n",
    "LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018.\n",
    "\n",
    "There are minor architectural differences. Compared to GPT-3, LLaMA\n",
    "\n",
    "- uses SwiGLU[7] activation function instead of GeLU;\n",
    "- uses rotary positional embeddings[8] instead of absolute positional embedding;\n",
    "- uses root-mean-squared layer-normalization[9] instead of standard layer-normalization.[10]\n",
    "- increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n",
    "\n",
    "Training datasets\n",
    "\n",
    "LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\n",
    "\n",
    "LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\n",
    "\n",
    "-     Webpages scraped by CommonCrawl\n",
    "-     Open source repositories of source code from GitHub\n",
    "-     Wikipedia in 20 different languages\n",
    "-     Public domain books from Project Gutenberg\n",
    "-     The LaTeX source code for scientific papers uploaded to ArXiv\n",
    "-     Questions and answers from Stack Exchange websites\n",
    "\n",
    "Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6Q6LAYhjJZf"
   },
   "source": [
    "Write your prompt below, and call it ```prompt_task2```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OdD7vsOQhYNb"
   },
   "outputs": [],
   "source": [
    "prompt_task2 = 'can you provide me with an extractive summary of the following article in 3-5 sentences?' + task2_article \n",
    "\n",
    "prompt_template_task2 = f'''\n",
    "\n",
    "USER: {prompt_task2}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "lhQXws0Aigaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USER: can you provide me with an extractive summary of the following article in 3-5 sentences?\n",
      "LLaMA\n",
      "\n",
      "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023.\n",
      "\n",
      "For the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters.\n",
      "LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\n",
      "\n",
      "In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.\n",
      "\n",
      "LLaMA-2\n",
      "\n",
      "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA.\n",
      "Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4]\n",
      "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5]\n",
      "The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
      "\n",
      "LLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat.\n",
      "In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases.\n",
      "However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative\n",
      "(known for maintaining the Open Source Definition).[6]\n",
      "\n",
      "Architecture\n",
      "\n",
      "LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018.\n",
      "\n",
      "There are minor architectural differences. Compared to GPT-3, LLaMA\n",
      "\n",
      "- uses SwiGLU[7] activation function instead of GeLU;\n",
      "- uses rotary positional embeddings[8] instead of absolute positional embedding;\n",
      "- uses root-mean-squared layer-normalization[9] instead of standard layer-normalization.[10]\n",
      "- increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n",
      "\n",
      "Training datasets\n",
      "\n",
      "LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\n",
      "\n",
      "LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\n",
      "\n",
      "-     Webpages scraped by CommonCrawl\n",
      "-     Open source repositories of source code from GitHub\n",
      "-     Wikipedia in 20 different languages\n",
      "-     Public domain books from Project Gutenberg\n",
      "-     The LaTeX source code for scientific papers uploaded to ArXiv\n",
      "-     Questions and answers from Stack Exchange websites\n",
      "\n",
      "Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n",
      "\n",
      "\n",
      "ASSISTANT:\n",
      "Here is an extractive summary of the article LLaMA in 3-5 sentences:\n",
      "\n",
      "LLaMA is a family of autoregressive large language models (LLMs) released by Meta AI starting in February 2023. The model's performance on most NLP benchmarks exceeded that of GPT-3, and the largest model was competitive with state-of-the-art models such as PaLM and Chinchilla. LLaMA is trained using a noncommercial license, and its weights were leaked to the public via BitTorrent within a week of release. Meta released several models in July 2023 as Llama 2, including three model sizes: 7, 13, and 70 billion parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =    85.54 ms /   170 runs   (    0.50 ms per token,  1987.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1683.61 ms /  1048 tokens (    1.61 ms per token,   622.47 tokens per second)\n",
      "llama_print_timings:        eval time =  5469.97 ms /   169 runs   (   32.37 ms per token,    30.90 tokens per second)\n",
      "llama_print_timings:       total time =  7500.62 ms\n"
     ]
    }
   ],
   "source": [
    "response_task2 = lcpp_llm(\n",
    "    prompt=prompt_template_task2,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response_task2[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbHAysAbjjnt"
   },
   "source": [
    "# Task 3: machine translation with zero-shot prompting  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtzFDZ5OnV90"
   },
   "source": [
    "Task 3: Write a prompt to get llama2 to translate the Wikipedia article above to French if you are English, else to your native language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Msbu8qvKnoRv"
   },
   "source": [
    "Write your prompt below, and call it ```prompt_task3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kIRTWZLbnwkm"
   },
   "outputs": [],
   "source": [
    "prompt_task3 = 'Can you translate this article to Persian' + task2_article\n",
    "prompt_template_task3 = f'''\n",
    "\n",
    "USER: {prompt_task3}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b8dqBVRloGgq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USER: Can you translate this article to Persian\n",
      "LLaMA\n",
      "\n",
      "LLaMA (Large Language Model Meta AI) is a family of autoregressive large language models (LLMs), released by Meta AI starting in February 2023.\n",
      "\n",
      "For the first version of LLaMA, four model sizes were trained: 7, 13, 33, and 65 billion parameters.\n",
      "LLaMA's developers reported that the 13B parameter model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 (with 175B parameters) and that the largest model was competitive with state of the art models such as PaLM and Chinchilla.[1] Whereas the most powerful LLMs have generally been accessible only through limited APIs (if at all), Meta released LLaMA's model weights to the research community under a noncommercial license.[2] Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.[3]\n",
      "\n",
      "In July 2023, Meta released several models as Llama 2, using 7, 13 and 70 billion parameters.\n",
      "\n",
      "LLaMA-2\n",
      "\n",
      "On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA.\n",
      "Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.[4]\n",
      "The model architecture remains largely unchanged from that of LLaMA-1 models, but 40% more data was used to train the foundational models.[5]\n",
      "The accompanying preprint[5] also mentions a model with 34B parameters that might be released in the future upon satisfying safety targets.\n",
      "\n",
      "LLaMA-2 includes both foundational models and models fine-tuned for dialog, called LLaMA-2 Chat.\n",
      "In further departure from LLaMA-1, all models are released with weights, and are free for many commercial use cases.\n",
      "However, due to some remaining restrictions, the description of LLaMA as open source has been disputed by the Open Source Initiative\n",
      "(known for maintaining the Open Source Definition).[6]\n",
      "\n",
      "Architecture\n",
      "\n",
      "LLaMA uses the transformer architecture, the standard architecture for language modeling since 2018.\n",
      "\n",
      "There are minor architectural differences. Compared to GPT-3, LLaMA\n",
      "\n",
      "- uses SwiGLU[7] activation function instead of GeLU;\n",
      "- uses rotary positional embeddings[8] instead of absolute positional embedding;\n",
      "- uses root-mean-squared layer-normalization[9] instead of standard layer-normalization.[10]\n",
      "- increases context length from 2K (Llama 1) tokens to 4K (Llama 2) tokens between.\n",
      "\n",
      "Training datasets\n",
      "\n",
      "LLaMA's developers focused their effort on scaling the model's performance by increasing the volume of training data, rather than the number of parameters, reasoning that the dominating cost for LLMs is from doing inference on the trained model rather than the computational cost of the training process.\n",
      "\n",
      "LLaMA 1 foundational models were trained on a data set with 1.4 trillion tokens, drawn from publicly available data sources, including:[1]\n",
      "\n",
      "-     Webpages scraped by CommonCrawl\n",
      "-     Open source repositories of source code from GitHub\n",
      "-     Wikipedia in 20 different languages\n",
      "-     Public domain books from Project Gutenberg\n",
      "-     The LaTeX source code for scientific papers uploaded to ArXiv\n",
      "-     Questions and answers from Stack Exchange websites\n",
      "\n",
      "Llama 2 foundational models were trained on a data set with 2 trillion tokens. This data set was curated to remove Web sites that often disclose personal data of people. It also upsamples sources considered trustworthy.[5] Llama 2 - Chat was additionally fine-tuned on 27,540 prompt-response pairs created for this project, which performed better than larger but lower-quality third-party datasets. For AI alignment, reinforcement learning with human feedback (RLHF) was used with a combination of 1,418,091 Meta examples and seven smaller datasets. The average dialog depth was 3.9 in the Meta examples, 3.0 for Anthropic Helpful and Anthropic Harmless sets, and 1.0 for five other sets, including OpenAI Summarize, StackExchange, etc.\n",
      "\n",
      "\n",
      "ASSISTANT:\n",
      "\n",
      "The article you provided is not available in Persian, but I can certainly help you translate it! Here's the translation of the article to Persian:\n",
      "\n",
      "عنوان مقاله: لاما (LLaMA) - یک خانواده از MODEL های زبانی برای سایت های Meta\n",
      "\n",
      "لاما (LLaMA) is a family of autoregressive language models released by Meta AI starting in February 2023. The developers of LLaMA reported that the model's performance on most NLP benchmarks exceeded that of the much larger GPT-3 and was competitive with state-of-the-art models such as PaLM and Chinchilla. Unlike other powerful language models, which have been available only through limited APIs, Meta released LLaMA's model weights to the research community under a noncommercial license. Within a week of LLaMA's release, its weights were leaked to the public on 4chan via BitTorrent.\n",
      "\n",
      "In July\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =   130.07 ms /   256 runs   (    0.51 ms per token,  1968.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =  1719.13 ms /  1042 tokens (    1.65 ms per token,   606.12 tokens per second)\n",
      "llama_print_timings:        eval time =  8300.77 ms /   255 runs   (   32.55 ms per token,    30.72 tokens per second)\n",
      "llama_print_timings:       total time = 10553.39 ms\n"
     ]
    }
   ],
   "source": [
    "response_task3 = lcpp_llm(\n",
    "    prompt=prompt_template_task3,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response_task3[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c51-BocoX5Z"
   },
   "source": [
    "# Task 4: named entity recognition, one and few-shot prompting, JSON output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRt_bLDcpOOu"
   },
   "source": [
    "Task 4: Write a prompt to get llama2 to tag named entities in the following sentence as 'ORG' if organization, 'DATE' if date, 'NUM' if number, and 'MODEL' if an AI model, and to output the result in JSON format. Use a few examples to explain llama2 what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "J44m7lsZpdlM"
   },
   "outputs": [],
   "source": [
    "task4_sentence = \"On July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7K1KYhSrRjl"
   },
   "source": [
    "Write your prompt below, and call it ```prompt_task4```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "r83XLDIMrX7b"
   },
   "outputs": [],
   "source": [
    "prompt_task4 = 'extreact named entities in this sentence and tag them with ORG if organization, DATE if date, NUM if number and MODEL if AI model and format the result in a json format' + task4_sentence + \"you van use this example: sentence = The first version of DALL-E was announced in January 2021 by OpenAI and has had two other releases , output: [{'entitiy':DALL-E, 'label': MODEL},{'entity': January 2021, 'label': DATE},{'entity': OPenAI, 'label':ORG},{'entity':2,'label':NUM}]\"\n",
    "prompt_template_task4 = f'''\n",
    "\n",
    "USER: {prompt_task4}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_task4 = 'extreact named entities in this sentence and tag them with ORG if organization, DATE if date, NUM if number and MODEL if AI model and format the result in a json format' + task4_sentence + \"you van use this example: sentence1 = The first version of DALL-E was announced in January 2021 by OpenAI and has had two other releases , output1: [{'entitiy':DALL-E, 'label': MODEL},{'entity': January 2021, 'label': DATE},{'entity': OPenAI, 'label':ORG},{'entity':2,'label':NUM}], sentence2= Anthropic developed Claude-2 in 2023 with 100k context length, output2 = [{'entitiy':Claude-2, 'label': MODEL},{'entity': 2023, 'label': DATE},{'entity': Anthropic, 'label':ORG},{'entity':100k,'label':NUM}], sentence3 = GPT-4 is a multimodal large language model trained by OpenAI and the fourth iterationof the generative pretrained transformer models in March 2023, output3 =[{'entitiy':GPT-4, 'label': MODEL},{'entity': March 2023, 'label': DATE},{'entity': OPenAI, 'label':ORG},{'entity':fourth,'label':NUM}]\"\n",
    "prompt_template_task4 = f'''\n",
    "\n",
    "USER: {prompt_task4}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "9q_vkes4sdE_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USER: extreact named entities in this sentence and tag them with ORG if organization, DATE if date, NUM if number and MODEL if AI model and format the result in a json formatOn July 18, 2023, in partnership with Microsoft, Meta announced LLaMA-2, the next generation of LLaMA. Meta trained and released LLaMA-2 in three model sizes: 7, 13, and 70 billion parameters.you van use this example: sentence1 = The first version of DALL-E was announced in January 2021 by OpenAI and has had two other releases , output1: [{'entitiy':DALL-E, 'label': MODEL},{'entity': January 2021, 'label': DATE},{'entity': OPenAI, 'label':ORG},{'entity':2,'label':NUM}], sentence2= Anthropic developed Claude-2 in 2023 with 100k context length, output2 = [{'entitiy':Claude-2, 'label': MODEL},{'entity': 2023, 'label': DATE},{'entity': Anthropic, 'label':ORG},{'entity':100k,'label':NUM}], sentence3 = GPT-4 is a multimodal large language model trained by OpenAI and the fourth iterationof the generative pretrained transformer models in March 2023, output3 =[{'entitiy':GPT-4, 'label': MODEL},{'entity': March 2023, 'label': DATE},{'entity': OPenAI, 'label':ORG},{'entity':fourth,'label':NUM}]\n",
      "\n",
      "ASSISTANT:\n",
      "Hello! I'd be happy to help you with that. Here are the named entities in your sentences and their corresponding labels:\n",
      "\n",
      "sentence1: [ {'entitiy': 'LLaMA-2', 'label': 'MODEL'}, {'entity': 'July 18, 2023', 'label': 'DATE'}, {'entity': 'Meta', 'label': 'ORG'}, {'entity': 'three', 'label': 'NUM'} ]\n",
      "sentence2: [ {'entitiy': 'Claude-2', 'label': 'MODEL'}, {'entity': '2023', 'label': 'DATE'}, {'entity': 'Anthropic', 'label': 'ORG'}, {'entity': '100k', 'label': 'NUM'} ]\n",
      "sentence3: [ {'entitiy': 'GPT-4', 'label': 'MODEL'}, {'entity': 'March 2023', 'label': 'DATE'}, {'entity': 'OpenAI', 'label': 'ORG'}, {'entity': 'fourth', 'label': 'NUM'} ]\n",
      "\n",
      "Please note that in sentence1, the entity 'July \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =   130.85 ms /   256 runs   (    0.51 ms per token,  1956.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   493.15 ms /   282 tokens (    1.75 ms per token,   571.83 tokens per second)\n",
      "llama_print_timings:        eval time =  7266.41 ms /   255 runs   (   28.50 ms per token,    35.09 tokens per second)\n",
      "llama_print_timings:       total time =  8289.82 ms\n"
     ]
    }
   ],
   "source": [
    "response_task4 = lcpp_llm(\n",
    "    prompt=prompt_template_task4,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response_task4[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6F0xoUTs8eA"
   },
   "source": [
    "How did Llama2 do at the task? Try to experiment with both one-shot and few-shot prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfYXQkG1tO1L"
   },
   "source": [
    "# Task 5: dialogue act tagging, one and few-shot prompting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIRnQMGYtX01"
   },
   "source": [
    "Task 5: Write a prompt to get llama2 to tag dialogue acts in the following conversation, using your favourite dialogue act tagset, and to output the result in JSON format. Use a few examples to explain llama2 what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "1o2M0ofottvt"
   },
   "outputs": [],
   "source": [
    "task5_conversation = f'''\n",
    "A: . . . I need to travel in May.\n",
    "B: And, what day in May did you want to travel?\n",
    "A: OK uh I need to be there for a meeting that’s from the 12th to the 15th.\n",
    "B: And you’re flying into what city?\n",
    "A: Seattle.\n",
    "B: And what time would you like to leave Pittsburgh?\n",
    "A: Uh hmm I don’t think there’s many options for non-stop.\n",
    "B: Right. There’s three non-stops today.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTyTJP58uXL-"
   },
   "source": [
    "Write your prompt below, and call it ```prompt_task5```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "GZhJ1lv5uhNN"
   },
   "outputs": [],
   "source": [
    "prompt_task5 = f\"\"\"\n",
    "I need help with dialogue act tagging, when given a conversation, each utterance should be tagged with one of these relevant tags and the output should be returned in JSON format:\n",
    "'qestion' if it's a question\n",
    "'request' if it's a request for sth\n",
    "'greet' if it's a greeting\n",
    "'accept' if it indicates acceptance or agreement\n",
    "'state' if it provides a information about something\n",
    "'answer' response to a question\n",
    "\n",
    "for example the tags for these sequences can be defined like this:\n",
    "\n",
    "Dialogue 1:\n",
    "A: Hi\n",
    "B: Hi\n",
    "A: Where are you?\n",
    "B: near to campus\n",
    "A: coming to see you\n",
    "Output1:\n",
    "[\n",
    "    {{\"utterance\": \"Hi\", \"dialogue_act\": \"greet\"}},\n",
    "    {{\"utterance\": \"Hi\", \"dialogue_act\": \"greet\"}},\n",
    "    {{\"utterance\": \"Where are you?\", \"dialogue_act\": \"question\"}},\n",
    "    {{\"utterance\": \"near to campus\", \"dialogue_act\": \"answer\"}},\n",
    "    {{\"utterance\": \"coming to see you\", \"dialogue_act\": \"state\"}}\n",
    "]\n",
    "\n",
    "Dialogue 1:\n",
    "A: Hi\n",
    "B: Hello\n",
    "A: I need a bus ticket to Liverpool\n",
    "B: when do you want to go?\n",
    "A: tommorow before 12\n",
    "B: alright, I am looking for tickets now.\n",
    "Output1:\n",
    "[\n",
    "    {{\"utterance\": \"Hi\", \"dialogue_act\": \"greet\"}},\n",
    "    {{\"utterance\": \"Hello\", \"dialogue_act\": \"greet\"}},\n",
    "    {{\"utterance\": \"I need a bus ticket to Liverpool?\", \"dialogue_act\": \"request\"}},\n",
    "    {{\"utterance\": \"when do you want to go?\", \"dialogue_act\": \"question\"}},\n",
    "    {{\"utterance\": \"tommorow before 12\", \"dialogue_act\": \"answer\"}},\n",
    "    {{\"utterance\": \"alright, I am looking for tickets now.\", \"dialogue_act\": \"statement\"}}\n",
    "]\n",
    "\n",
    "based on these examples, i need you to tag this dialogue: {task5_conversation}\n",
    "\n",
    " \"\"\"\n",
    "prompt_template_task5 = f'''\n",
    "\n",
    "USER: {prompt_task5}\n",
    "\n",
    "ASSISTANT:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ljdXu4kJvzjx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "USER: \n",
      "I need help with dialogue act tagging, when given a conversation, each utterance should be tagged with one of these relevant tags and the output should be returned in JSON format:\n",
      "'qestion' if it's a question\n",
      "'request' if it's a request for sth\n",
      "'greet' if it's a greeting\n",
      "'accept' if it indicates acceptance or agreement\n",
      "'state' if it provides a information about something\n",
      "'answer' response to a question\n",
      "\n",
      "for example the tags for these sequences can be defined like this:\n",
      "\n",
      "Dialogue 1:\n",
      "A: Hi\n",
      "B: Hi\n",
      "A: Where are you?\n",
      "B: near to campus\n",
      "A: coming to see you\n",
      "Output1:\n",
      "[\n",
      "    {\"utterance\": \"Hi\", \"dialogue_act\": \"greet\"},\n",
      "    {\"utterance\": \"Hi\", \"dialogue_act\": \"greet\"},\n",
      "    {\"utterance\": \"Where are you?\", \"dialogue_act\": \"question\"},\n",
      "    {\"utterance\": \"near to campus\", \"dialogue_act\": \"answer\"},\n",
      "    {\"utterance\": \"coming to see you\", \"dialogue_act\": \"state\"}\n",
      "]\n",
      "\n",
      "Dialogue 1:\n",
      "A: Hi\n",
      "B: Hello\n",
      "A: I need a bus ticket to Liverpool\n",
      "B: when do you want to go?\n",
      "A: tommorow before 12\n",
      "B: alright, I am looking for tickets now.\n",
      "Output1:\n",
      "[\n",
      "    {\"utterance\": \"Hi\", \"dialogue_act\": \"greet\"},\n",
      "    {\"utterance\": \"Hello\", \"dialogue_act\": \"greet\"},\n",
      "    {\"utterance\": \"I need a bus ticket to Liverpool?\", \"dialogue_act\": \"request\"},\n",
      "    {\"utterance\": \"when do you want to go?\", \"dialogue_act\": \"question\"},\n",
      "    {\"utterance\": \"tommorow before 12\", \"dialogue_act\": \"answer\"},\n",
      "    {\"utterance\": \"alright, I am looking for tickets now.\", \"dialogue_act\": \"statement\"}\n",
      "]\n",
      "\n",
      "based on these examples, i need you to tag this dialogue: \n",
      "A: . . . I need to travel in May.\n",
      "B: And, what day in May did you want to travel?\n",
      "A: OK uh I need to be there for a meeting that’s from the 12th to the 15th.\n",
      "B: And you’re flying into what city?\n",
      "A: Seattle.\n",
      "B: And what time would you like to leave Pittsburgh?\n",
      "A: Uh hmm I don’t think there’s many options for non-stop.\n",
      "B: Right. There’s three non-stops today.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "ASSISTANT:\n",
      "Hi! Sure, I'd be happy to help you with that! Here are the dialogue acts for each utterance in your given conversation:\n",
      "\n",
      "[\n",
      "    {\"utterance\": \"I need to travel in May.\", \"dialogue_act\": \"request\"},\n",
      "    {\"utterance\": \"And, what day in May did you want to travel?\", \"dialogue_act\": \"question\"},\n",
      "    {\"utterance\": \"OK uh I need to be there for a meeting that’s from the 12th to the 15th.\", \"dialogue_act\": \"answer\"},\n",
      "    {\"utterance\": \"And you’re flying into what city?\", \"dialogue_act\": \"question\"},\n",
      "    {\"utterance\": \"Seattle.\", \"dialogue_act\": \"answer\"},\n",
      "    {\"utterance\": \"And what time would you like to leave Pittsburgh?\", \"dialogue_act\": \"question\"},\n",
      "    {\"utterance\": \"Uh hmm I don’t think there’s many options for non-stop.\", \"dialogue_act\": \"state\"}\n",
      "]\n",
      "\n",
      "Please let me know if this is what you were looking for!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6322.82 ms\n",
      "llama_print_timings:      sample time =   127.69 ms /   254 runs   (    0.50 ms per token,  1989.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   436.23 ms /   143 tokens (    3.05 ms per token,   327.81 tokens per second)\n",
      "llama_print_timings:        eval time =  7581.35 ms /   253 runs   (   29.97 ms per token,    33.37 tokens per second)\n",
      "llama_print_timings:       total time =  8535.72 ms\n"
     ]
    }
   ],
   "source": [
    "response_task5 = lcpp_llm(\n",
    "    prompt=prompt_template_task5,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.2,\n",
    "    top_k=50,\n",
    "    stop = ['USER:'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "\n",
    "print(response_task5[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8-kbvBzwD7-"
   },
   "source": [
    "How did llama2 do?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
